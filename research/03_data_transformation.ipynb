{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Omar\\\\Desktop\\\\Omar_Files\\\\Python_Analysis\\\\EndToEndMLProjectGenderClassification\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Omar\\\\Desktop\\\\Omar_Files\\\\Python_Analysis\\\\EndToEndMLProjectGenderClassification'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransfornmationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    drop_cols:str\n",
    "    lblenc_cols:str\n",
    "    ordinal_cols:str\n",
    "    trans_cols:str\n",
    "    numerical_cols:str\n",
    "    target_cols:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EndToEndMLProjectGenderClassification.constants import *\n",
    "from EndToEndMLProjectGenderClassification.utils.common import read_yaml,create_directories\n",
    "from src.EndToEndMLProjectGenderClassification.utils.common import remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH,\n",
    "                 schema_filepath=SCHEMA_FILE_PATH) -> None:\n",
    "        \n",
    "        self.config=read_yaml(config_filepath)\n",
    "        self.params=read_yaml(params_filepath)\n",
    "        self.schema=read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransfornmationConfig:\n",
    "        config=self.config.data_transformation\n",
    "        schema=self.schema\n",
    "    \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config  = DataTransfornmationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            drop_cols=schema.DROP_COLUMNS,\n",
    "            lblenc_cols=schema.LABL_ENCODING,\n",
    "            ordinal_cols=schema.ORDINAL_ENCODING,\n",
    "            trans_cols=schema.TRANSFORM_FEATURES,\n",
    "            numerical_cols=schema.NUMERICAL_FEATURES,\n",
    "            target_cols=schema.TARGET_COLUMN\n",
    "        )\n",
    "\n",
    "        return data_transformation_config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,OneHotEncoder,OrdinalEncoder,PowerTransformer\n",
    "from EndToEndMLProjectGenderClassification import logger\n",
    "#from EndToEndMLProjectGenderClassification.utils.common import get_size\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import janitor\n",
    "from imblearn.combine import SMOTETomek,SMOTEENN\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransfornmation:\n",
    "    def __init__(self,config:DataTransfornmationConfig):\n",
    "        self.config= config\n",
    "\n",
    "    def data_preperation(self):\n",
    "        df=pd.read_csv(self.config.data_path)\n",
    "        df=df.drop(self.config.drop_cols,axis=1)\n",
    "        df[\"veh_value\"]=df[\"veh_value\"]*10000\n",
    "        df=df[df[\"veh_value\"]>0]\n",
    "        logger.info(\"Dropping unneccesary cols and rows ==> Done\") \n",
    "\n",
    "\n",
    "        df=df.sort_values(by=self.config.ordinal_cols).reset_index().drop(\"index\",axis=1)\n",
    "        logger.info(\"sorting cols ==> Done\") \n",
    "        \n",
    "        #df[self.config.trans_cols]=remove_outliers(self.config.trans_cols)\n",
    "        #logger.info(\"remove_outliers ==> Done\")\n",
    "\n",
    "        for col in df.select_dtypes(include=\"object\"):\n",
    "            df[col]=LabelEncoder().fit_transform(df[col])\n",
    "        logger.info(\"data LabelEncoder Done\")\n",
    "\n",
    "        train_set,test_set=train_test_split(df,test_size=0.2,random_state=42)\n",
    "\n",
    "        train_set.to_csv(os.path.join(self.config.root_dir,\"train.csv\"),index=False)\n",
    "        test_set.to_csv(os.path.join(self.config.root_dir,\"test.csv\"),index=False)\n",
    "        logger.info(\"Splitting data into train and test subsets ==> Done\")   \n",
    "        return train_set,test_set         \n",
    "\n",
    "      \n",
    "    def train_test_transformation(self):      \n",
    "        train_set,test_set=self.data_preperation()\n",
    "        logger.info(\"got train_set,test_set from data_dropping()   ==> Done\")\n",
    "\n",
    "        train_set[self.config.trans_cols]=PowerTransformer(method=\"yeo-johnson\").fit_transform(train_set[self.config.trans_cols])\n",
    "        test_set[self.config.trans_cols]=PowerTransformer(method=\"yeo-johnson\").fit_transform(test_set[self.config.trans_cols])\n",
    "        logger.info(\"Apply PowerTransformer() to non normal data   ==> Done\")\n",
    "   \n",
    "        input_train_set,target_train_set=train_set.drop(self.config.target_cols,axis=1),train_set[self.config.target_cols]\n",
    "        input_test_set,target_test_set=test_set.drop(self.config.target_cols,axis=1),test_set[self.config.target_cols]\n",
    "        logger.info(\"define x,y for train and test subsets  ==> Done\")           \n",
    "\n",
    "        smt=SMOTEENN(random_state=42,sampling_strategy=\"minority\")\n",
    "        \n",
    "        input_train_set_final,target_train_set_final=smt.fit_resample(input_train_set,target_train_set)\n",
    "        input_test_set_final,target_test_set_final=smt.fit_resample(input_test_set,target_test_set)\n",
    "\n",
    "        logger.info(\"Apply SMOTEENN resampleing with sampling_strategy : minority  ==> Done\") \n",
    "\n",
    "        train_arr=np.c_[input_train_set_final,np.array(target_train_set_final)]\n",
    "        test_arr=np.c_[input_test_set_final,np.array(target_test_set_final)] \n",
    "\n",
    "        logger.info(\"Apply np.c_ to create train_arr and test_arr  ==> Done\") \n",
    "\n",
    "        with open(f\"{self.config.root_dir}/final_train.npy\", 'wb') as file_obj:\n",
    "           np.save(file_obj, train_arr)\n",
    "\n",
    "        with open(f\"{self.config.root_dir}/final_test.npy\", 'wb') as file_obj:\n",
    "           np.save(file_obj, test_arr) \n",
    "\n",
    "        logger.info(\"saving train_arr and test_arr  ==> Done\")    \n",
    "\n",
    "        logger.info(\"Data Splitting is completed\")   \n",
    "\n",
    "              \n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "        logger.info(input_train_set.shape) \n",
    "        logger.info(target_train_set.shape) \n",
    "        logger.info(input_test_set.shape) \n",
    "        logger.info(target_test_set.shape)\n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "\n",
    "        logger.info(input_train_set_final.shape) \n",
    "        logger.info(target_train_set_final.shape) \n",
    "        logger.info(input_test_set_final.shape) \n",
    "        logger.info(target_test_set_final.shape)\n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "        logger.info(train_arr.shape) \n",
    "        logger.info(test_arr.shape)   \n",
    "    \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 20:23:29,354: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-30 20:23:29,358: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-30 20:23:29,363: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-10-30 20:23:29,365: INFO: common: created directory at: artifacts]\n",
      "[2024-10-30 20:23:29,366: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-10-30 20:23:29,456: INFO: 2498106966: Dropping unneccesary cols and rows ==> Done]\n",
      "[2024-10-30 20:23:29,497: INFO: 2498106966: sorting cols ==> Done]\n",
      "[2024-10-30 20:23:29,530: INFO: 2498106966: data LabelEncoder Done]\n",
      "[2024-10-30 20:23:29,779: INFO: 2498106966: Splitting data into train and test subsets ==> Done]\n",
      "[2024-10-30 20:23:29,862: INFO: 2498106966: Dropping unneccesary cols and rows ==> Done]\n",
      "[2024-10-30 20:23:29,902: INFO: 2498106966: sorting cols ==> Done]\n",
      "[2024-10-30 20:23:29,934: INFO: 2498106966: data LabelEncoder Done]\n",
      "[2024-10-30 20:23:30,182: INFO: 2498106966: Splitting data into train and test subsets ==> Done]\n",
      "[2024-10-30 20:23:30,183: INFO: 2498106966: got train_set,test_set from data_dropping()   ==> Done]\n",
      "[2024-10-30 20:23:30,287: INFO: 2498106966: Apply PowerTransformer() to non normal data   ==> Done]\n",
      "[2024-10-30 20:23:30,296: INFO: 2498106966: define x,y for train and test subsets  ==> Done]\n",
      "[2024-10-30 20:23:34,797: INFO: 2498106966: Apply SMOTEENN resampleing with sampling_strategy : minority  ==> Done]\n",
      "[2024-10-30 20:23:34,800: INFO: 2498106966: Apply np.c_ to create train_arr and test_arr  ==> Done]\n",
      "[2024-10-30 20:23:34,803: INFO: 2498106966: saving train_arr and test_arr  ==> Done]\n",
      "[2024-10-30 20:23:34,805: INFO: 2498106966: Data Splitting is completed]\n",
      "=========================\n",
      "[2024-10-30 20:23:34,806: INFO: 2498106966: (54242, 7)]\n",
      "[2024-10-30 20:23:34,806: INFO: 2498106966: (54242, 1)]\n",
      "[2024-10-30 20:23:34,807: INFO: 2498106966: (13561, 7)]\n",
      "[2024-10-30 20:23:34,808: INFO: 2498106966: (13561, 1)]\n",
      "=========================\n",
      "[2024-10-30 20:23:34,809: INFO: 2498106966: (16723, 7)]\n",
      "[2024-10-30 20:23:34,810: INFO: 2498106966: (16723, 1)]\n",
      "[2024-10-30 20:23:34,811: INFO: 2498106966: (4185, 7)]\n",
      "[2024-10-30 20:23:34,811: INFO: 2498106966: (4185, 1)]\n",
      "=========================\n",
      "[2024-10-30 20:23:34,812: INFO: 2498106966: (16723, 8)]\n",
      "[2024-10-30 20:23:34,813: INFO: 2498106966: (4185, 8)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransfornmation(config=data_transformation_config)\n",
    "    data_transformation.data_preperation()\n",
    "    data_transformation.train_test_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenderClassificationenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

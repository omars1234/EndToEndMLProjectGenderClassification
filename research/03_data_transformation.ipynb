{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Omar\\\\Desktop\\\\Omar_Files\\\\Python_Analysis\\\\EndToEndMLProjectGenderClassification'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Omar\\\\Desktop\\\\Omar_Files\\\\Python_Analysis\\\\EndToEndMLProjectGenderClassification'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransfornmationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    drop_cols:str\n",
    "    lblenc_cols:str\n",
    "    ordinal_cols:str\n",
    "    trans_cols:str\n",
    "    numerical_cols:str\n",
    "    target_cols:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EndToEndMLProjectGenderClassification.constants import *\n",
    "from EndToEndMLProjectGenderClassification.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH,\n",
    "                 schema_filepath=SCHEMA_FILE_PATH) -> None:\n",
    "        \n",
    "        self.config=read_yaml(config_filepath)\n",
    "        self.params=read_yaml(params_filepath)\n",
    "        self.schema=read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransfornmationConfig:\n",
    "        config=self.config.data_transformation\n",
    "        schema=self.schema\n",
    "    \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config  = DataTransfornmationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            drop_cols=schema.DROP_COLUMNS,\n",
    "            lblenc_cols=schema.LABL_ENCODING,\n",
    "            ordinal_cols=schema.ORDINAL_ENCODING,\n",
    "            trans_cols=schema.TRANSFORM_FEATURES,\n",
    "            numerical_cols=schema.NUMERICAL_FEATURES,\n",
    "            target_cols=schema.TARGET_COLUMN\n",
    "        )\n",
    "\n",
    "        return data_transformation_config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,OneHotEncoder,OrdinalEncoder,PowerTransformer\n",
    "from EndToEndMLProjectGenderClassification import logger\n",
    "#from EndToEndMLProjectGenderClassification.utils.common import get_size\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import janitor\n",
    "from imblearn.combine import SMOTETomek,SMOTEENN\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransfornmation:\n",
    "    def __init__(self,config:DataTransfornmationConfig):\n",
    "        self.config= config\n",
    "\n",
    "    def data_preperation(self):\n",
    "        df=pd.read_csv(self.config.data_path)\n",
    "        df=df.drop(self.config.drop_cols,axis=1)\n",
    "        df=df[df[\"veh_value\"]>0]\n",
    "        logger.info(\"Dropping unneccesary cols and rows ==> Done\") \n",
    "\n",
    "        df=df.sort_values(by=self.config.ordinal_cols).reset_index().drop(\"index\",axis=1)\n",
    "        logger.info(\"sorting cols ==> Done\") \n",
    "\n",
    "        col_to_move = \"gender\"\n",
    "        if col_to_move in df.columns:\n",
    "            df = df[[col for col in df.columns if col != col_to_move] + [col_to_move]] \n",
    "        logger.info(\"Moving the Target Feature to be the last column in the data set ==> Done\")\n",
    "\n",
    "        for col in df.select_dtypes(include=\"object\"):\n",
    "            df[col]=LabelEncoder().fit_transform(df[col])\n",
    "        logger.info(\"data LabelEncoder Done\")\n",
    "\n",
    "        train_set,test_set=train_test_split(df,test_size=0.2,random_state=42)\n",
    "\n",
    "        train_set.to_csv(os.path.join(self.config.root_dir,\"train.csv\"),index=False)\n",
    "        test_set.to_csv(os.path.join(self.config.root_dir,\"test.csv\"),index=False)\n",
    "        logger.info(\"Splitting data into train and test subsets ==> Done\")   \n",
    "        return train_set,test_set         \n",
    "\n",
    "      \n",
    "\n",
    "    def train_test_transformation(self):      \n",
    "        train_set,test_set=self.data_preperation()\n",
    "        logger.info(\"got train_set,test_set from data_dropping()   ==> Done\")\n",
    "\n",
    "        train_set[self.config.trans_cols]=PowerTransformer(method=\"yeo-johnson\").fit_transform(train_set[self.config.trans_cols])\n",
    "        test_set[self.config.trans_cols]=PowerTransformer(method=\"yeo-johnson\").fit_transform(test_set[self.config.trans_cols])\n",
    "        logger.info(\"Apply PowerTransformer() to non normal data   ==> Done\")\n",
    "   \n",
    "        input_train_set,target_train_set=train_set.drop(self.config.target_cols,axis=1),train_set[self.config.target_cols]\n",
    "        input_test_set,target_test_set=test_set.drop(self.config.target_cols,axis=1),test_set[self.config.target_cols]\n",
    "        logger.info(\"define x,y for train and test subsets  ==> Done\")\n",
    "\n",
    "        input_train_set_arry=input_train_set\n",
    "        input_test_set_arry=input_test_set\n",
    "        logger.info(\"Apply StandardScaler().fit_transform on input train and transform on input test ==> Done\")     \n",
    "\n",
    "        smt=SMOTEENN(random_state=42,sampling_strategy=\"minority\")\n",
    "        \n",
    "        input_train_set_final,target_train_set_final=smt.fit_resample(input_train_set_arry,target_train_set)\n",
    "        input_test_set_final,target_test_set_final=smt.fit_resample(input_test_set_arry,target_test_set)\n",
    "\n",
    "        logger.info(\"Apply SMOTEENN resampleing with sampling_strategy : minority  ==> Done\") \n",
    "\n",
    "        train_arr=np.c_[input_train_set_final,np.array(target_train_set_final)]\n",
    "        test_arr=np.c_[input_test_set_final,np.array(target_test_set_final)] \n",
    "\n",
    "        logger.info(\"Apply np.c_ to create train_arr and test_arr  ==> Done\") \n",
    "\n",
    "        with open(f\"{self.config.root_dir}/final_train.npy\", 'wb') as file_obj:\n",
    "           np.save(file_obj, train_arr)\n",
    "\n",
    "        with open(f\"{self.config.root_dir}/final_test.npy\", 'wb') as file_obj:\n",
    "           np.save(file_obj, test_arr) \n",
    "\n",
    "        logger.info(\"saving train_arr and test_arr  ==> Done\")    \n",
    "\n",
    "        logger.info(\"Data Splitting is completed\")   \n",
    "\n",
    "              \n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "        logger.info(input_train_set.shape) \n",
    "        logger.info(target_train_set.shape) \n",
    "        logger.info(input_test_set.shape) \n",
    "        logger.info(target_test_set.shape)\n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "        logger.info(input_train_set_arry.shape) \n",
    "        logger.info(input_test_set_arry.shape) \n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "        logger.info(input_train_set_final.shape) \n",
    "        logger.info(target_train_set_final.shape) \n",
    "        logger.info(input_test_set_final.shape) \n",
    "        logger.info(target_test_set_final.shape)\n",
    "\n",
    "        print(\"=========================\")\n",
    "\n",
    "        logger.info(train_arr.shape) \n",
    "        logger.info(test_arr.shape)   \n",
    "    \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-11 07:23:05,940: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-11 07:23:05,944: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-11 07:23:05,950: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-10-11 07:23:05,952: INFO: common: created directory at: artifacts]\n",
      "[2024-10-11 07:23:05,954: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-10-11 07:23:06,055: INFO: 3203376931: Dropping unneccesary cols and rows ==> Done]\n",
      "[2024-10-11 07:23:06,102: INFO: 3203376931: sorting cols ==> Done]\n",
      "[2024-10-11 07:23:06,107: INFO: 3203376931: Moving the Target Feature to be the last column in the data set ==> Done]\n",
      "[2024-10-11 07:23:06,148: INFO: 3203376931: data LabelEncoder Done]\n",
      "[2024-10-11 07:23:06,455: INFO: 3203376931: Splitting data into train and test subsets ==> Done]\n",
      "[2024-10-11 07:23:06,544: INFO: 3203376931: Dropping unneccesary cols and rows ==> Done]\n",
      "[2024-10-11 07:23:06,620: INFO: 3203376931: sorting cols ==> Done]\n",
      "[2024-10-11 07:23:06,634: INFO: 3203376931: Moving the Target Feature to be the last column in the data set ==> Done]\n",
      "[2024-10-11 07:23:06,719: INFO: 3203376931: data LabelEncoder Done]\n",
      "[2024-10-11 07:23:07,166: INFO: 3203376931: Splitting data into train and test subsets ==> Done]\n",
      "[2024-10-11 07:23:07,171: INFO: 3203376931: got train_set,test_set from data_dropping()   ==> Done]\n",
      "[2024-10-11 07:23:07,360: INFO: 3203376931: Apply PowerTransformer() to non normal data   ==> Done]\n",
      "[2024-10-11 07:23:07,367: INFO: 3203376931: define x,y for train and test subsets  ==> Done]\n",
      "[2024-10-11 07:23:07,368: INFO: 3203376931: Apply StandardScaler().fit_transform on input train and transform on input test ==> Done]\n",
      "[2024-10-11 07:23:12,256: INFO: 3203376931: Apply SMOTEENN resampleing with sampling_strategy : minority  ==> Done]\n",
      "[2024-10-11 07:23:12,258: INFO: 3203376931: Apply np.c_ to create train_arr and test_arr  ==> Done]\n",
      "[2024-10-11 07:23:12,260: INFO: 3203376931: saving train_arr and test_arr  ==> Done]\n",
      "[2024-10-11 07:23:12,261: INFO: 3203376931: Data Splitting is completed]\n",
      "=========================\n",
      "[2024-10-11 07:23:12,262: INFO: 3203376931: (54242, 7)]\n",
      "[2024-10-11 07:23:12,263: INFO: 3203376931: (54242, 1)]\n",
      "[2024-10-11 07:23:12,264: INFO: 3203376931: (13561, 7)]\n",
      "[2024-10-11 07:23:12,264: INFO: 3203376931: (13561, 1)]\n",
      "=========================\n",
      "[2024-10-11 07:23:12,265: INFO: 3203376931: (54242, 7)]\n",
      "[2024-10-11 07:23:12,266: INFO: 3203376931: (13561, 7)]\n",
      "=========================\n",
      "[2024-10-11 07:23:12,267: INFO: 3203376931: (16687, 7)]\n",
      "[2024-10-11 07:23:12,267: INFO: 3203376931: (16687, 1)]\n",
      "[2024-10-11 07:23:12,268: INFO: 3203376931: (4182, 7)]\n",
      "[2024-10-11 07:23:12,271: INFO: 3203376931: (4182, 1)]\n",
      "=========================\n",
      "[2024-10-11 07:23:12,272: INFO: 3203376931: (16687, 8)]\n",
      "[2024-10-11 07:23:12,273: INFO: 3203376931: (4182, 8)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransfornmation(config=data_transformation_config)\n",
    "    data_transformation.data_preperation()\n",
    "    #data_transformation.data_encoding()\n",
    "    data_transformation.train_test_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenderClassificationenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
